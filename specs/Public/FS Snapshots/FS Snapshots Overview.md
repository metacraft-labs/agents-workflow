# Development Plan: Isolated Agent Runtimes for Agents-Workflow

## Main Objectives

* **Isolated Per-Agent Workspaces:** Extend the `agent-task` CLI command to spawn new agent runtimes (e.g. Codex CLI, Claude Code, Copilot, Gemini CLI, Goose, etc.) in *isolated* environments. Each agent runs against an independent, snapshot-based copy of the user's current Git working tree, ensuring deterministic execution and no cross-contamination between agents.
* **Copy-on-Write Efficiency:** Use copy-on-write (CoW) filesystem snapshot or container layering techniques so that creating an agent’s workspace is fast and storage-efficient. Snapshots/clones on CoW filesystems like ZFS and Btrfs are near-instant and consume no extra space initially. This allows us to cheaply provision multiple workspace copies from the same baseline.
* **Cross-Platform Support:** Design the solution to work on **Linux, macOS, and Windows** development environments. On Linux, leverage native filesystem capabilities (ZFS, Btrfs, or OverlayFS) for local isolation. On macOS/Windows (which lack such native CoW FS for our purposes), seamlessly orchestrate a persistent Linux VM or container where snapshots and Docker can run. The CLI should hide these details, giving a uniform experience.
* **Local and Remote Execution:** Support launching agent runtimes **locally** (on the same host) or on a **remote/virtual host** accessed via SSH. For example, a user on macOS might use a lightweight Linux VM (like Colima or a preconfigured cloud instance) as the execution host. The library will handle file synchronization and remote commands so that agents always run against the latest working copy state.

## Key Requirements and Environment Strategy

To meet the objectives, the implementation must adapt to different OS and filesystem environments. Below is the strategy for each scenario:

* **Linux (ZFS filesystem):** If the user's repository resides on a ZFS volume, use ZFS native snapshots and clones. Creating a snapshot and clone is extremely fast and initially consumes no additional disk space. The plan is to run `zfs snapshot <dataset>@<tag>` followed by `zfs clone <dataset>@<tag> <new_dataset>` to produce a writable clone of the current working tree. Once the clone is created, it will be mounted at the same path as the original repository within an isolated environment using Docker containers. This path preservation is critical to maintain the validity of any existing build artifacts, compiled binaries, or configuration files that contain absolute path references and would be invalidated if the working directory path changed. The agent process will run within this isolated namespace where it sees the cloned filesystem mounted at the familiar path, ensuring incremental builds are as efficient as possible. After execution, the clone can be destroyed to free space. This approach leverages both ZFS's CoW semantics to avoid copying data and Linux namespace isolation to maintain path consistency while ensuring complete isolation between agent runs. (We may require that the repository is on its own ZFS dataset, or document that snapshots will encompass the whole dataset containing the repo.)

* **Linux (Btrfs filesystem):** If the repo is on Btrfs, use Btrfs subvolume snapshots. Btrfs supports quick CoW snapshots of subvolumes. We will ensure the repository directory is a Btrfs subvolume (creating one if necessary). Then for each agent run, execute `btrfs subvolume snapshot <repo> <snapshot_dir>` to create a writable snapshot copy. This snapshot is a CoW clone sharing data with the original, so initial copy cost is minimal. The agent will run with its working dir set to the snapshot directory. Cleanup involves deleting the subvolume snapshot. Btrfs snapshots, like ZFS, are near-instant and efficient due to CoW.

* **Linux (Non-CoW filesystem, e.g. ext4/XFS):** If ZFS/Btrfs are not available, fall back to **OverlayFS or equivalent** to avoid full copies. Using OverlayFS, we can mount an overlay union with the original repo as the read-only lower layer and an empty tmpfs or directory as the upper layer (for writes), resulting in a merged mount that looks like a copy of the repo. The agent will operate in this merged mount, so any file modifications occur in the upper layer, leaving the real working tree untouched. Since any change to the original working copy will be visible to the agents under this setup, we temporarily make the working copy read-only while remaining agents are running. **Note:** OverlayFS mounting requires elevated privileges (CAP\_SYS\_ADMIN); the CLI will attempt to use `sudo` or instruct the user to run with proper rights if needed. If OverlayFS is not feasible, the last resort is a full recursive copy of the working tree (preferably using hard links or reflinks to save time/storage if supported). For example, on newer filesystems that support reflink (copy-on-write file copies), we can use `cp --reflink=auto` to quickly clone the tree at file block level. These fallbacks ensure compatibility even on vanilla ext4 systems, albeit with some performance cost.

* **macOS and Windows (via Persistent VM + Mutagen):** macOS and Windows hosts lack native Linux CoW filesystems and container support is indirect (e.g. Docker Desktop). The strategy here is to utilize a long-lived lightweight **Linux VM** (such as a Colima or Lima VM on macOS, or a Hyper-V/WSL2 VM on Windows) to serve as the agent execution environment. We will synchronize the user’s working copy into this VM using **Mutagen** or a similar bidirectional sync tool. Mutagen will keep a *real copy* of all files inside the VM, avoiding slow network filesystem access and yielding near-native IO performance. Within the VM, we then apply the same snapshot techniques as above (ZFS/Btrfs/Overlay as available). For instance, the VM disk could be formatted with Btrfs to leverage snapshotting. The CLI library will manage the VM and sync: e.g., on first use it can ensure the VM is running (possibly by invoking Colima or a custom lightweight Docker/LinuxKit VM), set up a Mutagen sync session between the host repo and the VM’s filesystem, and then trigger agent execution inside the VM (via SSH or `docker exec`). On Windows, if WSL2 is available, it could be used similarly (with P9 file shares or rsync, though Mutagen provides more control). The key requirement is that from the perspective of our tool, the remote VM acts like a Linux host with the latest code, accessible via SSH or Docker API. All snapshot and container operations happen in that Linux environment. The user experience on Mac/Windows will be nearly identical to Linux, aside from an initial setup of the sync mechanism. (We will document any one-time setup, like installing Mutagen or Colima, for developers on these platforms.). This setup will require the project to define a devcontainer image, offering a ZFS/btrfs workspace and all AI agent software pre-installed.

* **Remote Hosts via SSH:** In addition to local and VM scenarios, the design will allow **remote execution on any SSH-accessible Linux host**. A user might configure an IP/hostname (and credentials or keys) for a server or cloud VM that should run agent tasks. The library will then connect to that host and perform the same steps: create a snapshot of the code and run the agent. To get the code there, two approaches will be supported: (1) **On-demand sync** – e.g. use `rsync` or `scp` to copy the current working tree to the remote host into a designated path before each run (including uncommitted changes); or (2) **Persistent sync** – use Mutagen or a similar daemon to continuously sync the repository to the remote host (much like the local VM case). The second approach is more efficient for repeated runs, so we will integrate Mutagen support for any remote, not just local VMs (Mutagen can sync to an SSH target as well). In either case, the remote host should have the necessary tools (ZFS/Btrfs or overlay/Docker) installed. We will strive to make the remote orchestration as automated as possible (for example, the first run could check for required tools on the remote and print instructions if missing).

In summary, the solution will adapt to use the **best available isolation method per platform**: CoW snapshots on CoW filesystems, overlay or copy on others, and a VM+sync approach on non-Linux hosts. This maximizes performance and compatibility across environments.
